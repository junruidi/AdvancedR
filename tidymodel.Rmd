---
title: "Tidy Modeling with R"
output:
  html_document:
    toc: true
    toc_float: true
---

## 1. Why Tidyness is important for modeling

There are a few existing R packages that provide a unified interface to harmonize these heterogeneous modeling APIs, such as `caret` and `mlr`. The `tidymodels` framework is similar to these in adopting a unification of the function interface, as well as enforcing consistency in the function names and return values. It is different in its opinionated design goals and modeling implementation.

__Examples 1.1__

The `broom::tidy()` function is a tool for standardizing the structure of R objects, which returns many types of R objects in a more usable format. 

```{r, message=FALSE}
library(tidyverse)
# the result using map()

corr_res = map(mtcars %>% select(-mpg), cor.test, mtcars$mpg)
head(str(corr_res))
corr_res[[1]]
```

Then use `broom::tidy`
```{r}
library(broom)
tidy(corr_res[[1]])


corr_res %>% 
  # Convert each to a tidy format; `map_dfr()` stacks the data frames 
  map_dfr(tidy, .id = "predictor") %>% 
  ggplot(aes(x = fct_reorder(predictor, estimate))) + 
  geom_point(aes(y = estimate)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +
  labs(x = NULL, y = "Correlation with mpg")
```


## 2. Combing Base R models and the `tidyverse`

`dplyr`, `purrr`, `tidyr` 

__Example 2.1__

Fit separate models for each category of one variable


Step 1. separate the categories using `group_nest`
```{r}
data(crickets, package = "modeldata")
split_by_species = crickets  %>% group_nest(species) 

split_by_species
```

Step 2. Use `map` to train models

```{r}
model_by_species <- 
  split_by_species %>% 
  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))
model_by_species
```

Step 3. Use `tidy` to convert them to consistent data frame formats
```{r}
model_by_species %>% 
  mutate(coef = map(model, tidy)) %>% 
  select(species, coef) %>% 
  unnest(cols = c(coef))
```

## 3. Dataset Description

The Ames housing data set

```{r}
data(ames, package = "modeldata")
dim(ames)
```
```{r}
ggplot(ames, aes(x = Sale_Price)) + geom_histogram(bins = 50)
ggplot(ames, aes(x = Sale_Price)) + geom_histogram(bins = 50) + scale_x_log10()
```

```{r}
ames = ames %>% mutate(Sale_Price = log10(Sale_Price))
```

## 4. Data Spending

Steps to create useful model includes 

* parameter estimation

* model selection and tuning

* performance assessment

_data spending_: first consideration when modeling, how should the data be applied to these steps.

### 4.1 Common methods for splitting data

Training set: develop and optimize the model

Test set: final arbiter to determine the efficacy of the model


`inital_split()`

```{r}
library(rsample)
set.seed(123)

## Get the partitioning information
ames_split = initial_split(ames, prob = 0.8)

ames_train = training(ames_split)
ames_test = testing(ames_split)
```

_Stratified sampling_ needs to be used when there is dramatic class imbalance.  For regression problems, the outcome data can be artificially binned into quartiles and then stratified sampling conducted four separate times. This is an effective method for keeping the distributions of the outcome similar between the training and test set.

```{r}
set.seed(123)
ames_split <- initial_split(ames, prob = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)
```

## 5. Feature Engineering with `recipes`


### 5.1 Strucutre of a recipe
```{r}
library(tidymodels)
simple_ames =
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,
         data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_dummy(all_nominal())
simple_ames
```

1. `recipe` specifies the columns needed for the model
2. `step_log` declares which variable should be log transformed
3. `step_dummy`specifies converting all to dummy. The function `all_nominal` captures the names of any columns that are currently factor or character


### 5.2 Using recipes

`recipe`: defines the preprocessing,returns a recipe $\rightarrow$ 

`prep`: calculates the statistics from the training set, returns a recipe $\rightarrow$ 

`bake`: applies the preprocessing to datasets, returns a tibble 

### 5.3 Encoding qualitatitive data

`step_unknown`: change missing value to a dedicated factor

`step_novel`: allot new level for new factor level in new data

`step_other`:  analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of “other”, with a specific threshold that can be specified
